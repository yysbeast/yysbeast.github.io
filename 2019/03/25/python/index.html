<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>用SVM及KNN分类CIFAR10 | 朱一丹的作品集</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="(用SVM及KNN分类CIFAR10) 基本思路及算法SVM：假设一条直线为 W?X+b =0 为最优的分割线，把两类分开，而如何获取最优的直线，在SVM中最优分割面(超平面)就是：max [支持向量到超平面的最小距离]。KNN：通过计算欧氏距离，一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。">
<meta property="og:type" content="article">
<meta property="og:title" content="用SVM及KNN分类CIFAR10">
<meta property="og:url" content="http://yoursite.com/2019/03/25/python/index.html">
<meta property="og:site_name" content="朱一丹的作品集">
<meta property="og:description" content="(用SVM及KNN分类CIFAR10) 基本思路及算法SVM：假设一条直线为 W?X+b =0 为最优的分割线，把两类分开，而如何获取最优的直线，在SVM中最优分割面(超平面)就是：max [支持向量到超平面的最小距离]。KNN：通过计算欧氏距离，一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/03/25/python/1.png">
<meta property="og:image" content="http://yoursite.com/2019/03/25/python/2.png">
<meta property="og:image" content="http://yoursite.com/2019/03/25/python/3.png">
<meta property="og:image" content="http://yoursite.com/2019/03/25/python/4.png">
<meta property="og:image" content="http://yoursite.com/2019/03/25/python/5.png">
<meta property="og:image" content="http://yoursite.com/2019/03/25/python/6.png">
<meta property="og:image" content="http://yoursite.com/2019/03/25/python/7.png">
<meta property="og:image" content="http://yoursite.com/2019/03/25/python/8.png">
<meta property="og:image" content="http://yoursite.com/2019/03/25/python/9.png">
<meta property="og:updated_time" content="2019-03-25T15:29:05.525Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="用SVM及KNN分类CIFAR10">
<meta name="twitter:description" content="(用SVM及KNN分类CIFAR10) 基本思路及算法SVM：假设一条直线为 W?X+b =0 为最优的分割线，把两类分开，而如何获取最优的直线，在SVM中最优分割面(超平面)就是：max [支持向量到超平面的最小距离]。KNN：通过计算欧氏距离，一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。">
<meta name="twitter:image" content="http://yoursite.com/2019/03/25/python/1.png">
  
    <link rel="alternate" href="/atom.xml" title="朱一丹的作品集" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">朱一丹的作品集</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-python" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/03/25/python/" class="article-date">
  <time datetime="2019-03-25T15:18:53.000Z" itemprop="datePublished">2019-03-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      用SVM及KNN分类CIFAR10
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>(用SVM及KNN分类CIFAR10)</p>
<h1 id="基本思路及算法"><a href="#基本思路及算法" class="headerlink" title="基本思路及算法"></a>基本思路及算法</h1><p><strong>SVM</strong>：假设一条直线为 W?X+b =0 为最优的分割线，把两类分开，而如何获取最优的直线，在SVM中最优分割面(超平面)就是：max [支持向量到超平面的最小距离]。<br><strong>KNN</strong>：通过计算欧氏距离，一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。<br><img src="/2019/03/25/python/1.png" alt><br><a id="more"></a><br><img src="/2019/03/25/python/2.png" alt></p>
<h2 id="（一）SVM"><a href="#（一）SVM" class="headerlink" title="（一）SVM"></a>（一）SVM</h2><h4 id="1-目标函数"><a href="#1-目标函数" class="headerlink" title="1.目标函数"></a>1.目标函数</h4><p><img src="/2019/03/25/python/3.png" alt><br>xi是图像数据，展开为一个列向量[D x 1]。矩阵W（维度[K x D]）和向量b维度[K x1])是函数的参数，分为被称为权重和偏置。 Wxi得到的乘积是个列向量[K x 1]，K个值对应K类。 </p>
<h4 id="2-数据处理"><a href="#2-数据处理" class="headerlink" title="2.数据处理"></a>2.数据处理</h4><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def load_CIFAR_batch(filename):</span><br><span class="line">    print(filename)</span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        datadict = pickle.load(f, encoding='latin1')#反序列化对象。将文件数</span><br><span class="line">X = datadict[<span class="string">'data'</span>]                  据解析为一个Python对象。</span><br><span class="line">        Y = datadict[<span class="string">'labels'</span>]</span><br><span class="line">        X = X.reshape(<span class="number">10000</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>).transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).astype(“float”)</span><br><span class="line">        Y = np.array(Y)      #transpose改变的其实是索引号，类似排序</span><br><span class="line">        <span class="keyword">return</span> X, Y</span><br><span class="line"></span><br><span class="line">def load_CIFAR10(ROOT):</span><br><span class="line">    xs = []</span><br><span class="line">    ys = []  #列表</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">        f = os.path.join(ROOT, <span class="string">'data_batch_%d'</span> % (b,))</span><br><span class="line">        X, Y = load_CIFAR_batch(f)</span><br><span class="line">        xs.append(X)</span><br><span class="line">        ys.append(Y)</span><br><span class="line">    Xtr = np.concatenate(xs)  #数组拼接</span><br><span class="line">    Ytr = np.concatenate(ys)</span><br><span class="line">    del X, Y</span><br><span class="line">    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, <span class="string">'test_batch'</span>))</span><br><span class="line">    <span class="keyword">return</span> Xtr, Ytr, Xte, Yte</span><br></pre></td></tr></table></figure>
<h4 id="3-代价函数-hinge-loss"><a href="#3-代价函数-hinge-loss" class="headerlink" title="3.代价函数(hinge loss)"></a>3.代价函数(hinge loss)</h4><p><img src="/2019/03/25/python/4.png" alt><br><img src="/2019/03/25/python/5.png" alt></p>
<h4 id="4-正则化"><a href="#4-正则化" class="headerlink" title="4.正则化"></a>4.正则化</h4><p><img src="/2019/03/25/python/6.png" alt></p>
<p>正则化是一系列通过损失来使目标相加的技术，和前面的项形成竞争，前面的想适应你的训练数据，而这个想要让W呈现一种特殊的方式，所以有时在目标中他们会相互竞争。</p>
<h4 id="5-梯度下降"><a href="#5-梯度下降" class="headerlink" title="5.梯度下降"></a>5.梯度下降</h4><p>梯度下降法的计算过程就是沿梯度下降的方向求解极小值，由于CIFAR10数据集比较大，如果我们使用批量梯度下降的方法效率很低，所以我们这里使用小批量梯度下降。<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sample_index = np.random.choice(num_train, batch_num, replace=False) </span><br><span class="line">           X_batch = X[sample_index, :]</span><br><span class="line">       y_batch = y[sample_index] #每次迭代时随机取batch_num个作</span><br><span class="line">#训练集，小批量梯度下降</span><br></pre></td></tr></table></figure></p>
<h4 id="6-计算代价函数和梯度值"><a href="#6-计算代价函数和梯度值" class="headerlink" title="6.计算代价函数和梯度值"></a>6.计算代价函数和梯度值</h4><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def svm_cost_function(self, X, y, reg, delta):#代价函数（loss hinge）</span><br><span class="line">        <span class="string">""</span><span class="string">" cal loss</span></span><br><span class="line"><span class="string">        :param X: A numpy array of shape (N, D)</span></span><br><span class="line"><span class="string">        :param y: A numpy array of shape (N, )</span></span><br><span class="line"><span class="string">        :param reg: regularization strength</span></span><br><span class="line"><span class="string">        :param delta: margin</span></span><br><span class="line"><span class="string">        :return: loss, gred</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        num_train = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        scores = X.dot(self.W.T)  # N * C   wT*xi</span><br><span class="line">        correct_class_scores = scores[range(num_train), y]</span><br><span class="line">        margins = scores - correct_class_scores[:, np.newaxis] + delta #delta一般是1</span><br><span class="line">        margins = np.maximum(<span class="number">0</span>, margins)</span><br><span class="line">        # do not ignore it, because 'y - y + delta' &gt; 0, we should reset it to zeros</span><br><span class="line">        margins[range(num_train), y] = <span class="number">0</span></span><br><span class="line">        loss = np.sum(margins) / num_train + <span class="number">0.5</span> * reg * np.sum(self.W * self.W)</span><br><span class="line"></span><br><span class="line">        ground_true = np.zeros(margins.shape)  # N * C</span><br><span class="line">        ground_true[margins &gt; <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        sum_margins = np.sum(ground_true, axis=<span class="number">1</span>)</span><br><span class="line">        ground_true[range(num_train), y] -= sum_margins</span><br><span class="line">        gred = ground_true.T.dot(X) / num_train + reg * self.W</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, gred</span><br></pre></td></tr></table></figure>
<h4 id="7-自动化确定超参"><a href="#7-自动化确定超参" class="headerlink" title="7.自动化确定超参"></a>7.自动化确定超参</h4><p>对每一组参数分别使用训练集训练模型，然后用验证集来得到分类准确率来比较其泛化能力从而选出最优的参数，再根据参数预测出最佳模型。<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> regularization_strengths:</span><br><span class="line">        svm = SVM()</span><br><span class="line">        # X, y, reg, delta, learning_rate, batch_num, num_iter, debug</span><br><span class="line">        svm.train(X_train, y_train, j, <span class="number">1</span>, i, <span class="number">200</span>, <span class="number">1500</span>, True)</span><br><span class="line">        y_pred = svm.predict(X_val)</span><br><span class="line">        acc_val = np.mean(y_val == y_pred)</span><br><span class="line">        <span class="keyword">if</span> best_val &lt; acc_val:</span><br><span class="line">            best_val = acc_val</span><br><span class="line">            best_parameter = (i, j)</span><br></pre></td></tr></table></figure></p>
<h4 id="8-predict函数"><a href="#8-predict函数" class="headerlink" title="8.predict函数"></a>8.predict函数</h4><p>将每个测试集的X对每个分类计算分数，并将分数最高的作为分类<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def predict(self, X):</span><br><span class="line">  <span class="string">""</span><span class="string">" predict</span></span><br><span class="line"><span class="string">:param X: A numpy array of shape (N, D)</span></span><br><span class="line"><span class="string">:return: y_pred (A numpy array of shape (N, ))</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span></span><br><span class="line">scores = X.dot(self.W.T)</span><br><span class="line"></span><br><span class="line">y_pred = np.zeros(X.shape[<span class="number">0</span>])</span><br><span class="line">y_pred = np.argmax(scores, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure></p>
<h4 id="9-结果"><a href="#9-结果" class="headerlink" title="9.结果"></a>9.结果</h4><p>最终结果准确率大概在38%左右，曾经尝试用sklearn里的svm函数进行预测，但是数据量太大难以计算出结果。<br>以下是在迭代数不同的loss值的变化，迭代数较大时会出现数据的抖动，出现锯齿状的图像，在选择比较合适的迭代数时，曲线比较平滑<br><img src="/2019/03/25/python/7.png" alt><br><img src="/2019/03/25/python/8.png" alt></p>
<h4 id="10-分析"><a href="#10-分析" class="headerlink" title="10.分析"></a>10.分析</h4><p>最后结果准确率大约在38%左右，因为考虑到svm算法的特性，其实划分特征的时候，很容易引起误差，相比较隐层网络的准确率要低。</p>
<h2 id="（二）KNN"><a href="#（二）KNN" class="headerlink" title="（二）KNN"></a>（二）KNN</h2><h4 id="1-计算欧氏距离"><a href="#1-计算欧氏距离" class="headerlink" title="1.计算欧氏距离"></a>1.计算欧氏距离</h4><p><img src="/2019/03/25/python/9.png" alt><br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def compute_distances(self, X_test):</span><br><span class="line">        <span class="string">""</span><span class="string">"计算测试集和每个训练集的欧氏距离</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        向量化实现需转化公式后实现（单个循环不需要）</span></span><br><span class="line"><span class="string">        :param X_test: 测试集 numpy.ndarray</span></span><br><span class="line"><span class="string">        :return: 测试集与训练集的欧氏距离数组 numpy.ndarray</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        dists = np.zeros((X_test.shape[<span class="number">0</span>], self.X_train.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">        value_2xy = np.multiply(X_test.dot(self.X_train.T), <span class="number">-2</span>)</span><br><span class="line">        value_x2 = np.sum(np.square(X_test), axis=<span class="number">1</span>, keepdims=True)</span><br><span class="line">        value_y2 = np.sum(np.square(self.X_train), axis=<span class="number">1</span>)</span><br><span class="line">        dists = value_2xy + value_x2 + value_y2</span><br><span class="line">        <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure></p>
<h4 id="2-预测分类"><a href="#2-预测分类" class="headerlink" title="2.预测分类"></a>2.预测分类</h4><p>根据上述计算的欧氏距离，对于每一个需要预测的样本，我们取前K个最接近的训练数据的分类，然后将这些分类中个数最多的分类作为预测结果.<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def predict_label(self, dists, k):</span><br><span class="line">    <span class="string">""</span><span class="string">"选择前K个距离最近的标签，从这些标签中选择个数最多的作为预测分类</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param dists: 欧氏距离</span></span><br><span class="line"><span class="string">    :param k: 前K个分类</span></span><br><span class="line"><span class="string">    :return: 预测分类（向量）</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    y_pred = np.zeros(dists.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dists.shape[<span class="number">0</span>]):</span><br><span class="line">        # 取前K个标签</span><br><span class="line">        closest_y = self.y_train[np.argsort(dists[i, :])[:k]]</span><br><span class="line">        # 取K个标签中个数最多的标签</span><br><span class="line">        y_pred[i] = np.argmax(np.bincount(closest_y))</span><br><span class="line">    <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure></p>
<h4 id="3-交叉验证"><a href="#3-交叉验证" class="headerlink" title="3.交叉验证"></a>3.交叉验证</h4><p>在我们训练完整数据集之前，我们必须先确定超参的值，这里我是用交叉验证方法，该方法主要思路为将训练集分为n份，然后每份分别作为测试集，剩下的作为训练集来检验不同的超参的效果，它适用于数据集较少的情况，因为交叉验证训练次数比较多，所以n的值不宜取过大，我们一般取5-10就可以了；如果数据集够大，我们可以使用验证集的方式，即从训练集取一小部分作为验证集来检验不同超参的效果，这样我们所需的训练次数就会少很多了<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">def Cross_validation(X_train, y_train):</span><br><span class="line">    <span class="string">""</span><span class="string">"交叉验证，确定超参K，同时可视化K值</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param X_train: 训练集</span></span><br><span class="line"><span class="string">    :param y_train: 训练标签</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    num_folds = <span class="number">5</span></span><br><span class="line">    k_choices = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line">    k_accuracy = &#123;&#125;</span><br><span class="line">    # 将数据集分为5份</span><br><span class="line">    X_train_folds = np.array_split(X_train, num_folds)</span><br><span class="line">    y_train_folds = np.array_split(y_train, num_folds)</span><br><span class="line">    # 计算每种K值</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">        k_accuracy[k] = []</span><br><span class="line">        # 每个K值分别计算每份数据集作为测试集时的正确率</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(num_folds):</span><br><span class="line">            # 构建数据集</span><br><span class="line">            X_te = X_train_folds[index]</span><br><span class="line">            y_te = y_train_folds[index]</span><br><span class="line">            X_tr = np.reshape(X_train_folds[:index] + X_train_folds[index + <span class="number">1</span>:], (X_train.shape[<span class="number">0</span>] * (num_folds - <span class="number">1</span>) / num_folds, <span class="number">-1</span>))</span><br><span class="line">            y_tr = np.reshape(y_train_folds[:index] + y_train_folds[index + <span class="number">1</span>:], (X_train.shape[<span class="number">0</span>] * (num_folds - <span class="number">1</span>) / num_folds))</span><br><span class="line">            # 预测结果</span><br><span class="line">            classify = KNearestNeighbor()</span><br><span class="line">            classify.train(X_tr, y_tr)</span><br><span class="line">            y_te_pred = classify.predict(X_te, k=k)</span><br><span class="line">            accuracy = np.sum(y_te_pred == y_te) / float(X_te.shape[<span class="number">0</span>])</span><br><span class="line">            k_accuracy[k].append(accuracy)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k, accuracylist <span class="keyword">in</span> k_accuracy.items():</span><br><span class="line">        <span class="keyword">for</span> accuracy <span class="keyword">in</span> accuracylist:</span><br><span class="line">            print(<span class="string">"k = %d, accuracy = %.3f"</span> % (k, accuracy))</span><br></pre></td></tr></table></figure></p>
<h4 id="4-结果"><a href="#4-结果" class="headerlink" title="4.结果"></a>4.结果</h4><p>当我们确定了超参以后，我们就可以将完整的数据进行训练了，当K值取10左右的正确率为最高。由于KNN在数据集比较大的时候（图片的数据维度很大）计算量特别大，所以我将5000个数据作为完整训练集，500个数据作为测试集。在上述数据中得到的结果是正确率为28.2%，识别率比较低。</p>
<h2 id="（三）两种算法的比较"><a href="#（三）两种算法的比较" class="headerlink" title="（三）两种算法的比较"></a>（三）两种算法的比较</h2><p>通过两个方法的对比可以发现，两种方法的准确率都不是很高，这与算法性质有关，相比较神经网络训练数据越多拟合越高，这两个算法对于图形分类的精度就不够高。<br>两者相比的话，KNN没有训练的过程，只有到了需要决策时才会利用已有数据进行决策，而在这之前不会经历SVM所拥有的训练过程。而SVM是先利用训练数据进行训练得到一个目标函数，待需要时就只利用训练好的函数进行决策。<br>总的来说，SVM需要训练过程，预测效率高，KNN计算复杂度高，但是需要调的参比较小。</p>
<h2 id="（四）总结"><a href="#（四）总结" class="headerlink" title="（四）总结"></a>（四）总结</h2><p>在本次课程设计中，通过对CIFAR10图像进行分类，运用了SVM算法以及KNN算法进行分类，两种算法虽然最后的结果的准确率不是很高，这是由于算法性质决定的，在划分图像的过程中，这两种算法造成的误差会比较大。在实现分类的过程中，首先是处理数据，CIFAR10是，由于数据集很大，在实现KNN 的时候就会发现，明显处理速度会变慢，因此减少了数据集的规模。其次是在SVM算法中，算法比较难懂，公式较为复杂，理解起来有点儿吃力。<br>但在此次的过程中突然理解了一点机器学习的算法和实现之间的联系，通俗点来说，近似于算法思想引导建立数学模型去吻合数据，然后通过数据的训练和交叉验证得到拟合度较好的模型，再去用这个模型去测试测试集进行分类。并且两种方法经过对比后，对两种算法理解更加透彻深刻。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/03/25/python/" data-id="cjtolto1900046cvtgi2905wu" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/03/26/hexo/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          hexo博客的搭建和使用
        
      </div>
    </a>
  
  
    <a href="/2019/03/25/tank/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">网络游戏设计与开发课程课程设计</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/03/26/hexo/">hexo博客的搭建和使用</a>
          </li>
        
          <li>
            <a href="/2019/03/25/python/">用SVM及KNN分类CIFAR10</a>
          </li>
        
          <li>
            <a href="/2019/03/25/tank/">网络游戏设计与开发课程课程设计</a>
          </li>
        
          <li>
            <a href="/2019/03/25/ar/">基于AR的益智涂涂乐游戏设计与开发</a>
          </li>
        
          <li>
            <a href="/2019/03/24/modelunity/">三维视景仿真开发工具课程设计</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Yidan Zhu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>